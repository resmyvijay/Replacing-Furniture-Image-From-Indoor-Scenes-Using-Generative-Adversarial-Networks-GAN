# Furniture Replacement In Indoor Scenes
# Abstract:
One important aspect of interior decoration involves the proper adjustment of furniture items according to the available space and visual perceptions of people as per their satisfaction. Instead of physically moving furniture, we can use virtual ways of replacing furniture, thus saving us time and effort. Deep learning techniques using Convolutional Neural Networks (CNN) are useful in providing a virtual way of replacing existing furniture with new furniture in indoor scenes. However, most algorithms are focused on accurately reproducing missing regions, and do not sufficiently take into account the physical requirements of fitting the furniture naturally within an area of specified dimensions. This requires not only image reconstruction but also geometric matching to identify the correct position of the new furniture in the indoor space. This project proposes a Generative Adversarial Networks (GAN) based model to overcome the twin challenges of missing image area completion using existing image background (floor, wall and ceiling), and accurate positioning of new furniture image. This can be done using two GAN-based neural networks where one network learns the elements of the background and recreates the indoor scene without a furniture object and another GAN network assesses the positioning of a new furniture object within the indoor scene for accuracy in order to generate realistic furniture replacement within an indoor scene. This project employs existing deep learning architectures and models such as Mask R CNN, DeepLabv3+, EdgeConnect and ST-GAN to perform the virtual replacement of furniture. The project employs semantic segmentation techniques, mask creation, image inpainting using GAN, and virtual image replacement using GAN. The research identifies the improvements obtained by using a novel GAN architecture ST-GAN for virtual image replacement. ST-GAN is a combination of spatial transformer networks (STN) and GANs. STNs provide the geometric corrections to the image and GANs assesses the accuracy of the positioning of the image. By iterative use of STNs and GAN, a realistic composite image is created, that will have a correct positioning of a new furniture object in place of another furniture object. Thus, instead of physically replacing furniture, you can take a photo of an indoor scene with a furniture and replace the furniture with another furniture virtually. This will save the time and effort of physically moving objects. This will allow furnishing companies to provide a useful virtual shopping experience without manual labour and enable interior decorators to gain a better perception of the aesthetic effects of shifting furniture objects. The system is useful within a 2D space with front view images of furniture.

## Stages Of The Project
  ![image](https://user-images.githubusercontent.com/55789995/159106621-bd6a043c-1fe5-4ed7-a1a5-3cd66ff03a38.png)

## Image Semantic Segmentation:
Image semantic segmentation is the process of segmenting an image into various regions, with each region classified as an object belonging to a category. For example, an indoor scene image containing furniture and other objects such as umbrella, TV etc. will be segmented into various objects named as sofa, umbrella, TV and so on. The results are stored in a dictionary data structure which contains the coordinates of the various objects in the image, the number of objects detected, their names and masks for each object.

In this project, image semantic segmentation is done using Mask R CNN.  Mask R CNN is a Convolutional Neural Network that does object detection, segmentation and classification. Mask R CNN is based on Faster R CNN. Faster R CNN is based on R CNN. R CNN is a Region-Based Convolutional Neural Network. Mask R CNN consists of three stages. In the first stage, it uses a ResNet neural network architecture (ResNet 101) to detect objects within the image. Resnet 101 architecture is a convolutional neural network (CNN) with a depth of 101 layers. It is trained using images in the ImageNet database. The ImageNet database consists of more than a million images of various categories. So, it knows the features for many types of ordinary objects such as furniture objects. The Resnet 101 model can be used to detect furniture objects such as chair, couch, bed and dining table. For this, a trained model with weights “mask_rcnn_coco.h5” is downloaded and saved in a local directory. Using the trained model, objects can be detected from an input image. Once Resnet 101 identifies the various objects in the image, various regions are identified that contain features of the objects. This is called as Region Proposal Network. These regions called Regions of Interest (ROI) are then provided as input to the second stage. Next, the objects have to be classified.

In the second stage, the network uses a classifier to classify the various Region of Interest (ROI) into objects of various categories and also creates bounding boxes for each of the objects. The Regions of Interest (ROIs) in the previous stage will be of many different sizes for one object, but in the second stage, the size of each region is fixed. For object classification, the Mask R CNN is trained using the MS COCO Dataset. The MS Coco dataset is a dataset of over 3 million images with objects classified into 80 object categories. For this project, we use an already trained model that is trained to classify objects into various classes such as chair, couch, and dining table. 

In the third stage, Mask R CNN creates masks for each of the Regions of Interest detected for each object. Each of the mask segments the image at the pixel level. Thus, there are four outputs from the Mask R CNN. The outputs after the detection for indoor scene image and furniture image are stored in dictionaries named results_f and results_i respectively.

Each dictionary has keys for the bounding boxes, masks, class ids and scores. The keys are as follows: 
•	‘rois’: The bounding boxes or regions-of-interest (ROI) for detected objects
•	‘masks’: The masks for the detected objects
•	‘class_ids’: The class integers for the detected objects
•	‘scores’: The probability or confidence for each predicted class

## Image Inpainting

From the input indoor scene image, only the furniture object has to be removed keeping the rest of the scene intact. After the furniture object is removed, the missing region has to be recreated. This process is called Image Inpainting and is done by the Image Inpainting algorithm. The indoor scene image with the furniture object is provided as input to the Image Inpainter. 

Image Inpainter uses DeepLabv3 architecture to remove the furniture object DeepLabv3+ is a Convolutional Neural Network (CNN) using atrous convolution that is built on ResNet-101 architecture as the backbone. ResNet-101 is a deep learning model consisting of 101 layers and is pretrained with more than a million images from ImageNet database. So, it can detect and classify images of more than 1000 categories. 

Image Inpainting is the process by which the indoor scene image with the missing furniture object is recreated in order to fill in the missing region. In our case, the missing region is due to the removal of the furniture object. The image is then recreated in such a way that it appears exactly how it will be if the furniture is actually removed from the indoor scene. This means, the background is filled in with the colour and texture of the surrounding background next to the furniture object. This is done by using the EdgeConnect algorithm. 

## Edge Connect Algorithm

The EdgeConnect algorithm identifies the pixels of the missing region using the information provided by the region next to the edges of the furniture object that has been removed. First, the edges of the missing region are hallucinated. It uses Canny Edge Detector to detect and highlight the edges. This creates an Edge Map. 

Then, a CNN estimates randomly the pixel value of the missing region and a loss is calculated based on the pixel values surrounding the edges. This loss is then used to improve the estimate of the missing region pixel values, and finally the missing region is reconstructed almost realistically.
In order to get an accurate edge map, we use a Generative Adversarial Network (GAN). The GAN assesses if the edge map is accurate or not, thus leading to highly accurate results.  

Let’s see how Generative Adversarial Network (GAN) is used for Image Inpainting. A GAN consists of a generator and a discriminator. A generator generates random images to fill in the missing regions and sends it to the discriminator in order to fool the discriminator into believing that the image has been correctly inpainted. The discriminator’s job is to verify if the generator output is realistic or not. The discriminator is a pre-trained neural network trained on a million images from the Image Net database. Based on its training, it initially identifies the generator’s images as fake. So, the generator improves its network in order to generate better inpainted images. There are three losses that are considered: Style loss (Lstyle), Perceptual loss (Lperc) and Reconstruction loss (Lreconc) Based on these losses, the generator network improves to get better realistic images. 

The discriminator’s role is only to identify if the inpainted image is fake or real. Thus both the generator and the discriminator play a minimax game which ultimately leads to highly realistic inpainted images.

## Virtual Furniture Replacement

We have now removed the existing furniture from the indoor scene image and inpainted the indoor scene to fill in the missing regions. The final part is to insert the new furniture image into the indoor scene. 

Image replacement can be done in many ways. But, in order to achieve realistic image replacement, Generative Adversarial Networks (GANs) combined with Spatial Transformer Networks offer a realistic placement of an image within an indoor scene. The primary requirements for a realistic image are the accurate placement of the furniture object within the indoor scene. For example, the image should not be left hanging in the air or be too deep into the floor, but should be just touching the floor. This is taken care of by the combination of STNs with GANs. So, the algorithm for image replacement is based on ST-GAN, a Generative Adversarial Network (GAN) that uses a combination of Spatial Transformer Networks (STN) with GAN.

STNs do the geometric corrections within the geometric space, and GANs allow for identifying the correctness of the image placement based on supervised training over a dataset of indoor scene images with furniture.

ST-GAN consists of a generator that is an STN and a discriminator that has been trained on SUNCG dataset. The SUNCG dataset is a collection of 45,622 indoor scenes with 5 million 3D object instances from 37 categories. The new furniture image and the background inpainted image are given as inputs to the ST-GAN. In the GAN, the STN does a geometric transformation on the new furniture image to get the correct orientation, and this is passed to the discriminator of the GAN. The discriminator based on its training using the SUNCG dataset, identifies if the positioning of the new furniture image in the background image is accurate or not. This cycle is repeated 4 times, and the output image after the iterations is as close realistically to the final output desired in which the new furniture image is accurately positioned within the indoor scene image.

STNs are Convolutional Neural Networks (CNNs) that perform geometric corrections to an image. Based on a feedback, the STNs can learn better geometric corrections on an image. The feedback is given by a discriminator that has been trained on a dataset of indoor scene images and therefore, able to identify if the geometric correction to the furniture image is accurate or not. The aim here is to position the new furniture image correctly on the background image so that the resulting image looks realistic and natural.
This requires the mask of the furniture image, the furniture image and the background image which in our case is the inpainted image to be given as inputs to the GAN

The generator which is the STN receives both the background image (inpainted image) and the furniture image. The STN then calculates the geometric corrections required to be done on the furniture image based on the geometry of the object and the inpainted image. Thus, small corrections to the furniture image are iteratively done by the STN based on possible layouts of furniture objects in a room finally generating a composite image Icomp. 
The STN predicts a geometric correction Δp1 using a geometric prediction network. This network determines the correction based on the background image and the foreground furniture object image. This is based on the dimensions of the foreground furniture object and the dimensions of the indoor scene, the views of the object, and the semantic labelling of the various regions in the indoor scene layout. 

The composite image generated by the STN in each iteration is sent to a GAN. In each iteration, the geometric network is improved based on the loss obtained from the discriminator in the GAN. At the beginning, the initial generator G1 is trained based on a dataset. In the successive iterations, the generator is trained based on the loss obtained from the discriminator. So, the generator in ST-GAN resembles a stack of generators, and in each iteration i, a new generator network Gi is created with the new geometric update ∆pi and the previous generator is kept as it is. So, in every new iteration, only the new Generator Gi is updated. A new composite image Icomp (pi) based on the predicted state pi is send to the discriminator at each iteration. In the discriminator, this image is compared with actual data distributions of indoor scenes with furniture.

The discriminator in the GAN is trained on millions of indoor scene images in order to identify if the positioning of the furniture image is accurate or not. We use the SUNCG dataset consisting of more than 40,000 indoor scenes with objects belonging to more than 30 categories. The composite image from the STN generator is matched with the data from the datasets in order to identify if it is true or fake.
The loss function from the discriminator is fed to the generator in order to create more geometric corrections to the furniture image. This results in another iteration of corrections for the composite image. 
Again, the STN does geometric corrections to create a better composite image and feeds it to the discriminator. This min max game continues for four iterations, in order to generate a highly realistic image. Thus GAN in combination with STNs provides a framework to accurately position the new furniture image within the background image.
The output after Image Replacement is the final output which has the new furniture object replacing the existing furniture object in the indoor scene.

## Results:
<img width="495" alt="image" src="https://user-images.githubusercontent.com/55789995/159107030-c8cac1c4-2c8b-457d-8c91-42338f373448.png">
-----------------------------------------------------------------------------------------------------------------------------------------------------------
<img width="495" alt="image" src="https://user-images.githubusercontent.com/55789995/159107041-9096fd93-4552-4c04-840c-411dff7ac8c4.png">
-----------------------------------------------------------------------------------------------------------------------------------------------------------
<img width="495" alt="image" src="https://user-images.githubusercontent.com/55789995/159107246-53453185-45f3-4921-8a56-1ee8889c93c7.png">
-----------------------------------------------------------------------------------------------------------------------------------------------------------
<img width="495" alt="image" src="https://user-images.githubusercontent.com/55789995/159107056-a1133130-6bf3-4953-b28e-36467a8d06da.png">



